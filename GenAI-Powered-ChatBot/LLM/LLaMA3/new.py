from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_community.llms import Ollama
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

import streamlit as st
import os

# ğŸ“˜ Prompt Template with memory
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful, polite assistant skilled in data science. You answer clearly and with examples.",
        ),
        MessagesPlaceholder(variable_name="history"),  # for chat history
        ("user", "{question}"),
    ]
)

# ğŸ§  Memory to store chat
memory = ConversationBufferMemory(return_messages=True)

# ğŸ¤– Load local LLM (like Llama3)
llm = Ollama(model="llama3")

# ğŸ”— Connect prompt â†’ model â†’ output
output_parser = StrOutputParser()
chain = prompt | llm | output_parser

# ğŸŒ Streamlit UI
st.title("ğŸ” GenAI Chatbot with Memory + File Upload")

# ğŸ“ File Upload section
uploaded_file = st.file_uploader("Upload a text or CSV file", type=["txt", "csv"])
if uploaded_file is not None:
    content = uploaded_file.read().decode("utf-8")
    st.text_area("File Content", content, height=150)
    st.session_state["file_data"] = content

# ğŸ—£ï¸ Chat Input
input_text = st.text_input("Ask something below ğŸ‘‡")

# ğŸ§  Store memory in session
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []

# ğŸ’¬ Chatbot logic
if input_text:
    # Add memory to inputs
    full_prompt = {"question": input_text, "history": st.session_state["chat_history"]}

    # ğŸ”„ Get response
    response = chain.invoke(full_prompt)

    # Show response
    st.write("ğŸ¤– Bot:", response)

    # Save to history
    st.session_state["chat_history"].append(("user", input_text))
    st.session_state["chat_history"].append(("assistant", response))
